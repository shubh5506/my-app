
export const blogs = [
  {
    id: "azure-vs-python-ai-chatbot",
    title:
      "Azure Semantic Kernel (.NET) vs Python FastAPI for an AI Resume Chatbot",
    date: "Jan 2026",
    readTime: "6 min read",
    tags: [
      "Generative AI",
      "FastAPI",
      "Azure AI",
      "Semantic Kernel",
      "LangChain",
      "RAG"
    ],
    excerpt:
      "I built the same AI resume chatbot twiceâ€”once with Azure Semantic Kernel and once with Python FastAPI. Hereâ€™s what I learned about speed, cost, and long-term flexibility.",
    content: `
ğŸ” Why I Built My AI Resume Chatbot Twice (And What I Learned)

Most resumes are static.

You scroll.
You skim.
You leave.

I wanted something different.

So instead of a PDF, I built an AI-powered resume chatbot that lets recruiters talk to my experience:

â€œWhat cloud platforms do you know?â€
â€œShow me your GenAI projects.â€
â€œExplain your architecture decisions.â€

And it answers instantly using Retrieval-Augmented Generation (RAG).

But hereâ€™s the twistâ€¦

I didnâ€™t build it once.

I built it twice â€” using two completely different stacks â€” to see which one actually performs better in the real world.

â˜ï¸ Version 1 â€” Azure Semantic Kernel + .NET 6
<img src="/Azchat.png" style="width:100%; border-radius:12px; margin:20px 0;" />

Since Iâ€™ve spent years in C# and .NET, this felt like the natural starting point. Semantic Kernel made it easy to orchestrate prompts, plugins, and AI workflows directly inside Azure. Everything felt structured and â€œenterprise readyâ€ from day one.

âœ… What worked well

ğŸ§© Clean abstractions

ğŸ¢ Strong enterprise architecture patterns

âš¡ Quick initial prototyping

ğŸ”Œ Tight Azure integrations

âŒ Where it struggled

ğŸ’¸ Costs added up fast

âš™ï¸ Configuration felt heavy

ğŸ” Too many hidden layers

ğŸ”’ Vendor lock-in concerns

Over time, I noticed something: Azure saved me time upfrontâ€¦ but added complexity later. Every small change meant navigating multiple services and settings. It felt powerful â€” but heavy.

ğŸ Version 2 â€” FastAPI + LangChain
<img src="/rag.png" style="width:100%; border-radius:12px; margin:20px 0;" />

Then I rebuilt everything in Python.

Same features.
Same chatbot.
Same RAG pipeline.

Completely different experience.

And honestly?

It felt lighter immediately.

â­ What stood out

âš¡ Faster API responses

ğŸ’° Cheap hosting on Railway

ğŸ” Full transparency into every step

ğŸŒ Huge open-source ecosystem

ğŸ§  Simple embeddings + vector search
<img src="/trans.png" style="width:100%; border-radius:12px; margin:20px 0;" />

LangChain made document chunking, embeddings, and retrieval incredibly straightforward. Instead of fighting configurationâ€¦ I was just building. Iteration speed skyrocketed.

ğŸ“Š Head-to-Head Comparison

After deploying both versions, the differences were clear:

ğŸš€ Python API was faster

ğŸ’µ Deployment was cheaper

ğŸ” Iteration was easier

ğŸ”“ No vendor lock-in

Even as someone deeply experienced in .NET, I had to admit:
Python simply felt more natural for GenAI workloads.

ğŸ› ï¸ My Hybrid Architecture Going Forward

Hereâ€™s what I realized:
This isnâ€™t Python vs .NET.
Itâ€™s Python and .NET.

Each stack has strengths.

So Iâ€™m using Python + FastAPI for the core AI logic â€” where speed and flexibility matter most. And Iâ€™m using .NET for the frontend and other non-AI services â€” where I want strong typing and enterprise features.

ğŸ’­ Final Thoughts

Building the same product twice taught me more than any tutorial could.

â˜ï¸ Azure is powerful â€” but complex and expensive

ğŸ Python is fast â€” flexible and cost-effective

If youâ€™re building AI-first systems today, my advice is simple:
Stay lean. Stay open. Avoid lock-in. Optimize for iteration speed.

Because in GenAIâ€¦

The fastest team wins.
`
  }
];
